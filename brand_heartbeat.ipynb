{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from fancyimpute import KNN\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697, 73)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('brand_characteristics_data_file.xlsx')\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping the nan rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset after dropping all the rows with missing data:  (697, 73)\n",
      "Shape of the dataset after dropping all the rows with missing data:  (629, 73)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#df1 = df.dropna()\n",
    "#print(\"Shape of the dataset after dropping all the rows with missing data: \", np.shape(df))\n",
    "\n",
    "df2 = df.dropna(subset = ['Brand_Stature_C'])\n",
    "print(\"Shape of the dataset after dropping all the rows with missing data: \", np.shape(df2))\n",
    "missing_val = df2.Brand_Asset_C.isnull().sum()  # Checking if any of the 3 target variables have nan values\n",
    "print(missing_val) \n",
    "#df[df.Brand_Stature_C != 'Nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have used df2 as input to the model - because I did not want to have missing data in the y_train too !!\n",
    "# df2 is obtained after dropping rows with nan values in columns Brand_stature_C\n",
    "\n",
    "useless_cols = ['Category', 'Brand_id', 'Brand_name']\n",
    "data = df2.drop(useless_cols, axis=1)\n",
    "data.loc[:, 'Premium']\n",
    "data.Premium.replace(('Value', 'Middle', 'Premium'), (1, 2, 3), inplace=True) # 3 - Value == Budget brand\n",
    "onehot_v1 = pd.get_dummies(data.loc[:, 'Premium'], prefix='Premium')\n",
    "data = data.join(onehot_v1)\n",
    "data = data.drop('Premium', axis=1)\n",
    "\n",
    "onehot_v1 = pd.get_dummies(data.loc[:, 'Category_code'], prefix='Category_code')\n",
    "data = data.join(onehot_v1)\n",
    "data = data.drop('Category_code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "feature_list = \"Category_code, Age, Newness_to_category, \\\n",
    "Total_Users_pct, Total_Prefer_pct, Energized_Differentiation_C, Relevance_C, Esteem_C, \\\n",
    "Knowledge_C, Different_pct, Distinctive_pct, \\\n",
    "Unique_pct, Dynamic_pct, Innovative_pct, Leader_pct, Reliable_pct, High_quality_pct, \\\n",
    "Arrogant_pct, Authentic_pct, Best_Brand_pct, Carefree_pct, Cares_Customers_pct, Charming_pct, \\\n",
    "Daring_pct, Down_to_Earth_pct, Energetic_pct, Friendly_pct, Fun_pct, Gaining_In_Popularity_pct, \\\n",
    "Glamorous_pct, Good_Value_pct, Healthy_pct, Helpful_pct, High_Performance_pct, Independent_pct, \\\n",
    "Intelligent_pct, Kind_pct, Obliging_pct, Original_pct, Prestigious_pct, Progressive_pct, \\\n",
    "Restrained_pct, Rugged_pct, Sensuous_pct, Simple_pct, Social_pct, Socially_Responsible_pct, \\\n",
    "Straightforward_pct, Stylish_pct, Traditional_pct, Trendy_pct, Trustworthy_pct, \\\n",
    "Unapproachable_pct, Up_To_Date_pct, Upper_Class_pct, Visionary_pct, Worth_More_pct, \\\n",
    "Cutting_Edge_C, Classic_C, Superior_C, Chic_C, Customer_Centric_C, Outgoing_C, No_Nonsense_C, \\\n",
    "Distant_C, Regard_MS\"\n",
    "\n",
    "feature_list = feature_list.split(', ')\n",
    "#print imp_features\n",
    "print(len(feature_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  450\n",
      "Test dataset:  79\n",
      "Val dataset:  100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Age                            58.811160\n",
       "Newness_to_category            65.439750\n",
       "Total_Users_pct                33.939015\n",
       "Total_Prefer_pct               40.930202\n",
       "Energized_Differentiation_C     0.494656\n",
       "Relevance_C                     2.752115\n",
       "Esteem_C                        0.622283\n",
       "Knowledge_C                     3.579079\n",
       "Different_pct                   7.449110\n",
       "Distinctive_pct                10.459243\n",
       "Unique_pct                      7.748723\n",
       "Dynamic_pct                     7.604813\n",
       "Innovative_pct                  7.959433\n",
       "Leader_pct                     15.942557\n",
       "Reliable_pct                   17.516719\n",
       "High_quality_pct               18.048109\n",
       "Arrogant_pct                    4.902784\n",
       "Authentic_pct                   9.168356\n",
       "Best_Brand_pct                  9.052178\n",
       "Carefree_pct                    5.920752\n",
       "Cares_Customers_pct            11.567063\n",
       "Charming_pct                    6.076843\n",
       "Daring_pct                      6.603166\n",
       "Down_to_Earth_pct              19.176997\n",
       "Energetic_pct                   8.066489\n",
       "Friendly_pct                   15.383300\n",
       "Fun_pct                        14.970631\n",
       "Gaining_In_Popularity_pct       8.282274\n",
       "Glamorous_pct                   5.574626\n",
       "Good_Value_pct                 18.023955\n",
       "                                 ...    \n",
       "Worth_More_pct                  8.290556\n",
       "Cutting_Edge_C                  8.179883\n",
       "Classic_C                      12.689610\n",
       "Superior_C                     12.066335\n",
       "Chic_C                          7.095268\n",
       "Customer_Centric_C             13.054821\n",
       "Outgoing_C                     10.978340\n",
       "No_Nonsense_C                   8.662452\n",
       "Distant_C                       5.811949\n",
       "Regard_MS                       4.596299\n",
       "Premium_1                       0.233333\n",
       "Premium_2                       0.486667\n",
       "Premium_3                       0.275556\n",
       "Premium_MIddle                  0.004444\n",
       "Category_code_1                 0.086667\n",
       "Category_code_2                 0.104444\n",
       "Category_code_3                 0.075556\n",
       "Category_code_4                 0.035556\n",
       "Category_code_5                 0.077778\n",
       "Category_code_6                 0.020000\n",
       "Category_code_7                 0.051111\n",
       "Category_code_8                 0.133333\n",
       "Category_code_9                 0.042222\n",
       "Category_code_10                0.017778\n",
       "Category_code_11                0.037778\n",
       "Category_code_12                0.133333\n",
       "Category_code_13                0.031111\n",
       "Category_code_14                0.073333\n",
       "Category_code_15                0.026667\n",
       "Category_code_16                0.053333\n",
       "Length: 85, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = data[:450]\n",
    "val = data[450:550]\n",
    "test = data[550:]\n",
    "print(\"Train dataset: \", len(train))\n",
    "print(\"Test dataset: \", len(test))\n",
    "print(\"Val dataset: \", len(val))\n",
    "\n",
    "y_labels = ['Brand_Stature_C', 'Brand_Strength_C', 'Brand_Asset_C']\n",
    "\n",
    "y_train = train[y_labels]\n",
    "X_train = train.drop(y_labels, axis=1)\n",
    "\n",
    "\n",
    "y_val = val[y_labels]\n",
    "X_val = val.drop(y_labels, axis=1)\n",
    "\n",
    "y_test = test[y_labels]\n",
    "X_test = test.drop(y_labels, axis=1)\n",
    "\n",
    "mean_Xtrain = np.mean(X_train)\n",
    "display(mean_Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "impute_func = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "for i, feature in enumerate(feature_list[2:]):\n",
    "    #print(i)\n",
    "    X_train[feature] = impute_func.fit_transform(X_train[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      12.199670\n",
       "3      13.322060\n",
       "4      13.917360\n",
       "5       6.879766\n",
       "6      14.854470\n",
       "7      10.308100\n",
       "8       8.055678\n",
       "9       9.503695\n",
       "10     10.302430\n",
       "11      9.351686\n",
       "12      8.212419\n",
       "13      6.346408\n",
       "14      6.817083\n",
       "16      7.171248\n",
       "17      2.919549\n",
       "18      8.007871\n",
       "19      6.785540\n",
       "20      7.955283\n",
       "21      3.986943\n",
       "22      9.663995\n",
       "23      9.740601\n",
       "24     11.556740\n",
       "26     11.609190\n",
       "27      8.418119\n",
       "28      8.714547\n",
       "29     10.120280\n",
       "30     13.704010\n",
       "32      9.869622\n",
       "33      6.565214\n",
       "34     11.491470\n",
       "         ...    \n",
       "467    10.560410\n",
       "468    13.628270\n",
       "469    12.065790\n",
       "471    17.603451\n",
       "472    14.018850\n",
       "473    12.995370\n",
       "474     9.317093\n",
       "475    11.339310\n",
       "476    11.420750\n",
       "477    10.046480\n",
       "478    11.752410\n",
       "479    10.472030\n",
       "480    13.609890\n",
       "481     6.405635\n",
       "482     9.073104\n",
       "483     6.148084\n",
       "484    11.380370\n",
       "486    10.487360\n",
       "487     9.682291\n",
       "488    11.687460\n",
       "490    19.295490\n",
       "491     3.883129\n",
       "492    14.075120\n",
       "493    12.555390\n",
       "494     6.227262\n",
       "495    12.544120\n",
       "496    10.358460\n",
       "497     7.890357\n",
       "498    11.438730\n",
       "499    10.962520\n",
       "Name: Distinctive_pct, Length: 450, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#impute_func.fit_transform(train.loc[:,feature_list[7]].values.reshape(-1,1))\n",
    "\n",
    "X_train[feature_list[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beauty products', 'Beverages', 'Cars', \"Children's products\", 'Clothing products', 'Department stores', 'Financial services', 'Food and dining', 'Health products and services', 'Home design and decoration', 'Household Products', 'Media and entertainment', 'Sports and hobbies', 'Technology products and stores', 'Telecommunications', 'Travel services']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Beauty products': 52,\n",
       " 'Beverages': 66,\n",
       " 'Cars': 47,\n",
       " \"Children's products\": 19,\n",
       " 'Clothing products': 51,\n",
       " 'Department Stores': 15,\n",
       " 'Financial services': 39,\n",
       " 'Food and dining': 105,\n",
       " 'Health products and services': 27,\n",
       " 'Home design and decoration': 13,\n",
       " 'Household Products': 24,\n",
       " 'Media and entertainment': 103,\n",
       " 'Sports and hobbies': 21,\n",
       " 'Technology products and stores': 56,\n",
       " 'Telecommunications': 25,\n",
       " 'Travel services': 34}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list = \"Beauty products, Beverages, Cars, Children's products, Clothing products, Department stores, Financial services, Food and dining, Health products and services, Home design and decoration, Household Products, Media and entertainment, Sports and hobbies, Technology products and stores, Telecommunications, Travel services\"\n",
    "category_list = category_list.split(', ')\n",
    "print(category_list)\n",
    "\n",
    "a = dict(Counter(df.Category))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Checking for Nan values in X_train and summing them up\n",
    "#print(x_train.isnull().any())\n",
    "missing_val = X_train.isnull().sum()\n",
    "print(missing_val.sum()) \n",
    "\n",
    "# Prints a non zero value if there are any nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the missing data in all the columns - Errors due to category_code one hot encoding !!!!!!!!!!!\n",
    "\n",
    "\n",
    "# print(\"Printing the number of missing values in each feature...\")\n",
    "# for counter, feature in enumerate(feature_list):\n",
    "#     print(counter, feature)\n",
    "#     column = x_train[feature]\n",
    "#     print((column.isnull()==True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModeResult(mode=array([3.33250189]), count=array([1]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the imputation has been done properly - since there are a lot of missing values, \n",
    "# - mode(most repeated value) should be the mean value\n",
    "from scipy import stats\n",
    "stats.mode(X_train[feature_list[63]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ak/env/lib/python3.5/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/ak/env/lib/python3.5/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.25272078, -0.16124743, -0.26628422, ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633],\n",
       "       [ 0.552325  , -0.10185494,  1.23846537, ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633],\n",
       "       [ 0.80977742, -0.25906298,  1.05293888, ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633],\n",
       "       ...,\n",
       "       [-0.83089912, -0.95385888,  0.34013078, ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633],\n",
       "       [-1.03125413, -0.09775313, -0.60400836, ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633],\n",
       "       [ 2.57314826, -0.60252654,  0.        , ..., -0.28131244,\n",
       "        -0.16552118, -0.23735633]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Performing PCA\n",
    "\n",
    "#Standardizing the values before PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Xtrain_std = StandardScaler().fit_transform(X_train)\n",
    "display(Xtrain_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components before PCA:  (450, 85)\n",
      "Number of components after PCA:  (450, 85)\n",
      "N values which sum upto 0.9:  5\n",
      "Final size of the X_train after PCA:  (450, 5)\n"
     ]
    }
   ],
   "source": [
    "# Feature selection using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(X_train)\n",
    "\n",
    "print(\"Number of components before PCA: \", np.shape(X_train))\n",
    "print(\"Number of components after PCA: \", np.shape(principalComponents))\n",
    "\n",
    "#principalDf = pd.DataFrame(data = principalComponents\n",
    "#             , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "# Finding the most important features by choosing the features which have the highest variance\n",
    "\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "variance_ratio.sort(axis=0)\n",
    "variance_ratio = variance_ratio[::-1]\n",
    "\n",
    "# Taking the first n features which cause the variance to sum up to 0.9\n",
    "sum=0\n",
    "for i, item in enumerate(variance_ratio):\n",
    "    if(sum>=0.9):\n",
    "        break\n",
    "    sum+=item\n",
    "print(\"N values which sum upto 0.9: \", i)\n",
    "principal_list = variance_ratio[:i]\n",
    "\n",
    "# Picking out the top n components \n",
    "final_components = []\n",
    "for item in principal_list:\n",
    "    column_idx = variance_ratio.tolist().index(item)  # the variance_ratio is a numpy array array, \n",
    "                            # hence converting to a list and finding the indexes of the columns to be picked out \n",
    "    final_components.append(principalComponents[:,column_idx])\n",
    "X_train_final = np.transpose(np.array(final_components))\n",
    "print(\"Final size of the X_train after PCA: \", np.shape(X_train_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi RF score:  0.9576358397996646\n",
      "RF score:  0.9457278805071456\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model \n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "max_depth = 30\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=max_depth))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "\n",
    "regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,random_state=2)\n",
    "regr_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_multirf = regr_multirf.predict(X_test)\n",
    "y_rf = regr_rf.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of prediction accuracy\n",
    "score_multirf = regr_multirf.score(X_test, y_test)\n",
    "print(\"Multi RF score: \", score_multirf)\n",
    "\n",
    "score_rf = regr_rf.score(X_test, y_test)\n",
    "print(\"RF score: \", score_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    34.0\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students = [ ('jack', 34, 'Sydeny') ,\n",
    "             ('Riti', float('nan'), 'Delhi' ) ,\n",
    "             ('Aadi', 16, float('nan')) ]\n",
    "\n",
    "# Create a DataFrame object\n",
    "dfObj = pd.DataFrame(students, columns = ['Name' , 'Age', 'City'], index=['a', 'b', 'c'])\n",
    "dfObj\n",
    "\n",
    "dfObj = dfObj.dropna()\n",
    "\n",
    "\n",
    "dfObj.loc[:,'Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Observations"
    ]
   },
   "source": [
    "# Observations\n",
    "Number of missing rows : 79 (For total_users_pct and Total_Prefer_pct)\n",
    "                         68 (For other percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(200 * rng.rand(600, 1) - 100, axis=0)\n",
    "y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\n",
    "y += (0.5 - rng.rand(*y.shape))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=400, test_size=200, random_state=4)\n",
    "\n",
    "print(np.shape(y_train))\n",
    "max_depth = 30\n",
    "regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n",
    "                                                          max_depth=max_depth,\n",
    "                                                          random_state=0))\n",
    "regr_multirf.fit(X_train, y_train)\n",
    "\n",
    "regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,\n",
    "                                random_state=2)\n",
    "regr_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "y_multirf = regr_multirf.predict(X_test)\n",
    "y_rf = regr_rf.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "s = 50\n",
    "a = 0.4\n",
    "plt.scatter(y_test[:, 0], y_test[:, 1], edgecolor='k',\n",
    "            c=\"navy\", s=s, marker=\"s\", alpha=a, label=\"Data\")\n",
    "plt.scatter(y_multirf[:, 0], y_multirf[:, 1], edgecolor='k',\n",
    "            c=\"cornflowerblue\", s=s, alpha=a,\n",
    "            label=\"Multi RF score=%.2f\" % regr_multirf.score(X_test, y_test))\n",
    "plt.scatter(y_rf[:, 0], y_rf[:, 1], edgecolor='k',\n",
    "            c=\"c\", s=s, marker=\"^\", alpha=a,\n",
    "            label=\"RF score=%.2f\" % regr_rf.score(X_test, y_test))\n",
    "plt.xlim([-6, 6])\n",
    "plt.ylim([-6, 6])\n",
    "plt.xlabel(\"target 1\")\n",
    "plt.ylabel(\"target 2\")\n",
    "plt.title(\"Comparing random forests and the multi-output meta estimator\")\n",
    "plt.legend()\n",
    "plt.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
